
# ğŸ¤– Persona-Driven Document Intelligence

This project performs intelligent section extraction and summarization from PDF documents based on a given **persona** and **task** â€” entirely CPU-based. It uses spaCyâ€™s GloVe embeddings (300-d) for semantic similarity, keyword scoring, and summarization. Ideal for document intelligence systems constrained by size, performance, or offline requirements.

---

## ğŸš€ Features

- âœ… Uses **spaCy**â€™s GloVe (`en_core_web_md`) â€“ no PyTorch or transformers
- ğŸ§  Hybrid semantic scoring with cosine similarity, keyword overlap, and keyphrase matches
- ğŸ“„ PDF parsing with **pdfplumber**
- ğŸ”– Extracts meaningful page chunks using overlapping sentence segmentation
- ğŸ“ Generates **section headlines** and **summaries** using enhanced **TextRank**
- ğŸ“¦ Fully Dockerized and offline-capable
- âš¡ Efficient on CPU with <1GB memory usage

---

## ğŸ§± How It Works

1. **PDF Page Extraction**
   - Reads each page using `pdfplumber`
   - Cleans, strips footers/headers, filters low-quality pages

2. **Intelligent Page Filtering**
   - Ranks pages based on overlap with keywords/phrases from persona & task

3. **Text Chunking**
   - Breaks filtered pages into overlapping chunks (~400 words)

4. **Hybrid Chunk Scoring**
   - Computes similarity with persona+task embedding
   - Adds keyword & phrase match boost
   - Scores weighted into a `combined_score`

5. **Document Aggregation**
   - Ranks top 5 documents based on chunk scores
   - Extracts best page per document

6. **Headline + Summary Generation**
   - Extracts page-level summary using TextRank + clustering
   - Builds headlines based on keyphrase context relevance

---

## ğŸ“¦ Installation

### âœ… Requirements
- Python 3.8+
- `spaCy`, `pdfplumber`, `scikit-learn`, `numpy`, `networkx`, `tqdm`

### Install dependencies:
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_md
```

### `requirements.txt`:
```
numpy
tqdm
pdfplumber
spacy
scikit-learn
networkx
```

---

## ğŸ³ Docker Setup

### ğŸ”§ Build Docker Image
```bash
docker build --platform linux/amd64 -t
mysolutionname:somerandomidentifier
```

### â–¶ï¸ Run in Container
```bash
docker run --rm -v $(pwd)/input:/app/input -v $(pwd)/output:/app/output --
network none mysolutionname:somerandomidentifier
```

Ensure `input/input.json` contains:
```json
{
  "persona": { "role": "Data Scientist" },
  "job_to_be_done": { "task": "Summarize key findings" },
  "documents": [
    { "filename": "somefile.pdf" }
  ]
}
```

---

## ğŸ–¥ï¸ Local Usage

```bash
python main.py input/input.json output/output.json
```

---

## ğŸ“‚ Output Format

```json
{
  "metadata": {
    "input_documents": ["somefile.pdf"],
    "persona": "Data Scientist",
    "job_to_be_done": "Summarize key findings",
    "processing_timestamp": "2025-07-28T..."
  },
  "extracted_sections": [
    {
      "document": "somefile.pdf",
      "section_title": "Summary of Results: Key Findings",
      "importance_rank": 1,
      "page_number": 3
    }
  ],
  "subsection_analysis": [
    {
      "document": "somefile.pdf",
      "refined_text": "This study explored...",
      "page_number": 3
    }
  ]
}
```

---

## ğŸ§  About the Embeddings

- Uses `spaCy`'s `en_core_web_md` GloVe-based model (â‰ˆ120MB)
- L2-normalized vectors, cosine similarity = dot product
- Efficient caching (LRU with hash of `text[:200]`)

---

## ğŸ§¹ Memory Management

- Uses `SmartEmbedderManager.cleanup()` to:
  - Release spaCy model
  - Clear embedding cache
  - Force garbage collection

---


